float activationTANH(float x)
{
	return -1 + (2 / (1 + exp(-2 * x)));
}

float derivativeTANH(float x)
{
	return (1+x)*(1-x);
}

float activationLinear(float x)
{
	return x;
}

float derivativeLinear(float x)
{
	return 1;
}

float activationSigmoid(float x)
{
	float p = -1.0 * x;
	return 1.0 / (1.0 + exp(p));
}

float derivativeSigmoid(float x)
{
	return x * (1.0 - x);
}

float activation(int type, float x)
{
	switch(type)
	{
		case 0:
			return activationLinear(x);
		case 1:
			return activationTANH(x);
		case 2:
			return activationSigmoid(x);

	}
}

float derivative(int type, float x)
{
	switch(type)
	{
		case 0:
			return derivativeLinear(x);
		case 1:
			return derivativeTANH(x);
		case 2:
			return derivativeSigmoid(x);
	}
}

kernel void NetworkTrain(
    global read_only int *params,
    global write_only float *errors,
    global read_only int *layerIndex,
    global read_only int *layerCounts,
    global read_only int *weightIndex,
    global read_only float* input,
    global read_only float* ideal,
    global read_only float* weights,
    global write_only float *layerOutput,
    global write_only float *layerDelta,
    global write_only float *gradients,
    global read_only int *activationType
    )
{
	int inputSize = params[0];
    int outputSize = params[1];
    int layerCount = params[2];
    int neuronCount = params[3];
    int layerDeltaSize = params[4];
    int weightsSize = params[5];
	
	// part 1: forward pass
	int taskIndex = get_global_id(0);
	int taskOutputIndex = taskIndex * neuronCount;
	int taskInputIndex = taskIndex * inputSize;
	int taskIdealIndex = taskIndex * outputSize;
	int taskLayerDeltaIndex = taskIndex * layerDeltaSize;
	
    int sourceIndex = neuronCount - inputSize;

	// load the input into the layer output array, this feeds the first layer.
    for(int i=0;i<inputSize;i++)
        layerOutput[taskOutputIndex+(sourceIndex+i)] = input[taskInputIndex+i];

    for (int currentLayer = layerCount - 1; currentLayer > 0; currentLayer--)
    {
      int inputIndex = layerIndex[currentLayer];
      int outputIndex = layerIndex[currentLayer - 1];
      int inputSize = layerCounts[currentLayer];
      int outputSize = layerCounts[currentLayer - 1];
      int index = weightIndex[currentLayer - 1];

      for (int i = 0; i < outputSize; i++)
      {
        layerOutput[ taskOutputIndex+(i + outputIndex)] = weights[index++];
      }

      for (int x = 0; x < outputSize; x++)
      {
        float sum = 0;
        for (int y = 0; y < inputSize; y++)
        {
          sum += weights[index++] * layerOutput[taskOutputIndex+(inputIndex + y)];
        }
       
        float value = layerOutput[taskOutputIndex+(outputIndex + x)] + sum;
        value = activation(activationType[0], value);
        layerOutput[taskOutputIndex+(outputIndex + x)] = value;
      }
    }
 
	// part 2: backward pass
	int taskErrorIndex = taskIndex * outputSize;
	   
    float e = 0;
    for(int i=0;i<outputSize;i++)
    {
		float diff = ideal[taskIdealIndex+i] - layerOutput[taskOutputIndex+ i];
		errors[taskErrorIndex+i]=diff*diff;
		layerDelta[taskLayerDeltaIndex+i] = diff * derivative(activationType[0],layerOutput[taskOutputIndex+ i]);
    }
    
 
    for(int currentLevel = 0; (currentLevel<layerCount-1); currentLevel++)
    {
		int fromLayerIndex = layerIndex[currentLevel + 1];
        int toLayerIndex = layerIndex[currentLevel];
        int fromLayerSize = layerCounts[currentLevel + 1];
        int toLayerSize = layerCounts[currentLevel];
        
        // clear the to-deltas
        for (int i = 0; i < fromLayerSize; i++)
        {
			layerDelta[taskLayerDeltaIndex+fromLayerIndex + i] = 0;
        }
        
        int index = weightIndex[currentLevel];

        // handle thresholds
        for (int i = 0; i < toLayerSize; i++)
        {
			gradients[index++] += layerDelta[toLayerIndex+i];
        }       
        
 
        int gradientOffset = (taskIndex*weightsSize);

        for (int x = 0; x < toLayerSize; x++)
        {
			for (int y = 0; y < fromLayerSize; y++)
            {
				float value = layerOutput[taskOutputIndex+fromLayerIndex + y]
                            * layerDelta[taskLayerDeltaIndex+toLayerIndex + x];
                gradients[gradientOffset+index] += value;
                layerDelta[taskLayerDeltaIndex+fromLayerIndex + y] += weights[index]
                            * layerDelta[taskLayerDeltaIndex+toLayerIndex + x];
                index++;
            }
        }
        
        for (int i = 0; i < fromLayerSize; i++)
        {
			layerDelta[taskLayerDeltaIndex+fromLayerIndex + i] *= 
				derivative(activationType[currentLevel+1],layerOutput[taskOutputIndex+fromLayerIndex + i]);
        }
    }
}